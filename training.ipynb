{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoModel\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from sentence_transformers import util\n",
    "import google.generativeai as genai\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         ID                                         Resume_str  \\\n",
      "0  16852973           HR ADMINISTRATOR/MARKETING ASSOCIATE\\...   \n",
      "1  22323967           HR SPECIALIST, US HR OPERATIONS      ...   \n",
      "2  33176873           HR DIRECTOR       Summary      Over 2...   \n",
      "3  27018550           HR SPECIALIST       Summary    Dedica...   \n",
      "4  17812897           HR MANAGER         Skill Highlights  ...   \n",
      "\n",
      "                                         Resume_html Category  \n",
      "0  <div class=\"fontsize fontface vmargins hmargin...       HR  \n",
      "1  <div class=\"fontsize fontface vmargins hmargin...       HR  \n",
      "2  <div class=\"fontsize fontface vmargins hmargin...       HR  \n",
      "3  <div class=\"fontsize fontface vmargins hmargin...       HR  \n",
      "4  <div class=\"fontsize fontface vmargins hmargin...       HR  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "df = pd.read_csv(\"dataset/Resume.csv\")\n",
    "\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only useful columns\n",
    "df = df[['ID', 'Resume_str', 'Category']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python3.11\\Lib\\site-packages\\torch\\utils\\_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Python3.11\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python3.11\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained embedding model\n",
    "embed_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Convert resume text to embeddings\n",
    "df['resume_embedding'] = df['Resume_str'].apply(lambda x: embed_model.encode(x, convert_to_tensor=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "embeddings\n",
      "embeddings.word_embeddings\n",
      "embeddings.position_embeddings\n",
      "embeddings.token_type_embeddings\n",
      "embeddings.LayerNorm\n",
      "embeddings.dropout\n",
      "encoder\n",
      "encoder.layer\n",
      "encoder.layer.0\n",
      "encoder.layer.0.attention\n",
      "encoder.layer.0.attention.self\n",
      "encoder.layer.0.attention.self.query\n",
      "encoder.layer.0.attention.self.key\n",
      "encoder.layer.0.attention.self.value\n",
      "encoder.layer.0.attention.self.dropout\n",
      "encoder.layer.0.attention.output\n",
      "encoder.layer.0.attention.output.dense\n",
      "encoder.layer.0.attention.output.LayerNorm\n",
      "encoder.layer.0.attention.output.dropout\n",
      "encoder.layer.0.intermediate\n",
      "encoder.layer.0.intermediate.dense\n",
      "encoder.layer.0.intermediate.intermediate_act_fn\n",
      "encoder.layer.0.output\n",
      "encoder.layer.0.output.dense\n",
      "encoder.layer.0.output.LayerNorm\n",
      "encoder.layer.0.output.dropout\n",
      "encoder.layer.1\n",
      "encoder.layer.1.attention\n",
      "encoder.layer.1.attention.self\n",
      "encoder.layer.1.attention.self.query\n",
      "encoder.layer.1.attention.self.key\n",
      "encoder.layer.1.attention.self.value\n",
      "encoder.layer.1.attention.self.dropout\n",
      "encoder.layer.1.attention.output\n",
      "encoder.layer.1.attention.output.dense\n",
      "encoder.layer.1.attention.output.LayerNorm\n",
      "encoder.layer.1.attention.output.dropout\n",
      "encoder.layer.1.intermediate\n",
      "encoder.layer.1.intermediate.dense\n",
      "encoder.layer.1.intermediate.intermediate_act_fn\n",
      "encoder.layer.1.output\n",
      "encoder.layer.1.output.dense\n",
      "encoder.layer.1.output.LayerNorm\n",
      "encoder.layer.1.output.dropout\n",
      "encoder.layer.2\n",
      "encoder.layer.2.attention\n",
      "encoder.layer.2.attention.self\n",
      "encoder.layer.2.attention.self.query\n",
      "encoder.layer.2.attention.self.key\n",
      "encoder.layer.2.attention.self.value\n",
      "encoder.layer.2.attention.self.dropout\n",
      "encoder.layer.2.attention.output\n",
      "encoder.layer.2.attention.output.dense\n",
      "encoder.layer.2.attention.output.LayerNorm\n",
      "encoder.layer.2.attention.output.dropout\n",
      "encoder.layer.2.intermediate\n",
      "encoder.layer.2.intermediate.dense\n",
      "encoder.layer.2.intermediate.intermediate_act_fn\n",
      "encoder.layer.2.output\n",
      "encoder.layer.2.output.dense\n",
      "encoder.layer.2.output.LayerNorm\n",
      "encoder.layer.2.output.dropout\n",
      "encoder.layer.3\n",
      "encoder.layer.3.attention\n",
      "encoder.layer.3.attention.self\n",
      "encoder.layer.3.attention.self.query\n",
      "encoder.layer.3.attention.self.key\n",
      "encoder.layer.3.attention.self.value\n",
      "encoder.layer.3.attention.self.dropout\n",
      "encoder.layer.3.attention.output\n",
      "encoder.layer.3.attention.output.dense\n",
      "encoder.layer.3.attention.output.LayerNorm\n",
      "encoder.layer.3.attention.output.dropout\n",
      "encoder.layer.3.intermediate\n",
      "encoder.layer.3.intermediate.dense\n",
      "encoder.layer.3.intermediate.intermediate_act_fn\n",
      "encoder.layer.3.output\n",
      "encoder.layer.3.output.dense\n",
      "encoder.layer.3.output.LayerNorm\n",
      "encoder.layer.3.output.dropout\n",
      "encoder.layer.4\n",
      "encoder.layer.4.attention\n",
      "encoder.layer.4.attention.self\n",
      "encoder.layer.4.attention.self.query\n",
      "encoder.layer.4.attention.self.key\n",
      "encoder.layer.4.attention.self.value\n",
      "encoder.layer.4.attention.self.dropout\n",
      "encoder.layer.4.attention.output\n",
      "encoder.layer.4.attention.output.dense\n",
      "encoder.layer.4.attention.output.LayerNorm\n",
      "encoder.layer.4.attention.output.dropout\n",
      "encoder.layer.4.intermediate\n",
      "encoder.layer.4.intermediate.dense\n",
      "encoder.layer.4.intermediate.intermediate_act_fn\n",
      "encoder.layer.4.output\n",
      "encoder.layer.4.output.dense\n",
      "encoder.layer.4.output.LayerNorm\n",
      "encoder.layer.4.output.dropout\n",
      "encoder.layer.5\n",
      "encoder.layer.5.attention\n",
      "encoder.layer.5.attention.self\n",
      "encoder.layer.5.attention.self.query\n",
      "encoder.layer.5.attention.self.key\n",
      "encoder.layer.5.attention.self.value\n",
      "encoder.layer.5.attention.self.dropout\n",
      "encoder.layer.5.attention.output\n",
      "encoder.layer.5.attention.output.dense\n",
      "encoder.layer.5.attention.output.LayerNorm\n",
      "encoder.layer.5.attention.output.dropout\n",
      "encoder.layer.5.intermediate\n",
      "encoder.layer.5.intermediate.dense\n",
      "encoder.layer.5.intermediate.intermediate_act_fn\n",
      "encoder.layer.5.output\n",
      "encoder.layer.5.output.dense\n",
      "encoder.layer.5.output.LayerNorm\n",
      "encoder.layer.5.output.dropout\n",
      "pooler\n",
      "pooler.dense\n",
      "pooler.activation\n"
     ]
    }
   ],
   "source": [
    "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "# Print all available module names\n",
    "for name, _ in model.named_modules():\n",
    "    print(name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python3.11\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 110,592 || all params: 22,823,808 || trainable%: 0.4845\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('saved_model\\\\tokenizer_config.json',\n",
       " 'saved_model\\\\special_tokens_map.json',\n",
       " 'saved_model\\\\vocab.txt',\n",
       " 'saved_model\\\\added_tokens.json',\n",
       " 'saved_model\\\\tokenizer.json')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load base model & tokenizer\n",
    "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "# Define valid LoRA target modules dynamically\n",
    "target_modules = [f\"encoder.layer.{i}.attention.self.query\" for i in range(6)] + \\\n",
    "                 [f\"encoder.layer.{i}.attention.self.key\" for i in range(6)] + \\\n",
    "                 [f\"encoder.layer.{i}.attention.self.value\" for i in range(6)]\n",
    "\n",
    "# Apply PEFT with correct target modules\n",
    "peft_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=target_modules,  \n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"SEQ_CLS\"\n",
    ")\n",
    "\n",
    "# Integrate PEFT into the model\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()\n",
    "model.save_pretrained(\"saved_model\")\n",
    "tokenizer.save_pretrained(\"saved_model\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Match Score: 0.27143406867980957\n"
     ]
    }
   ],
   "source": [
    "def match_resume_to_job(resume_text, job_description):\n",
    "    resume_embedding = embed_model.encode(resume_text, convert_to_tensor=True)\n",
    "    job_embedding = embed_model.encode(job_description, convert_to_tensor=True)\n",
    "    return util.pytorch_cos_sim(resume_embedding, job_embedding).item()\n",
    "\n",
    "# Example: Match a resume with a job\n",
    "job_desc = \"Experienced Data Scientist with a strong background in Python, Machine Learning, and Data Analytics. Proficient in developing, training, and deploying machine learning models to drive business insights and automation. Skilled in handling large datasets, feature engineering, and implementing supervised and unsupervised learning algorithms. Adept at using libraries such as Pandas, NumPy, Scikit-learn, TensorFlow, and PyTorch to build scalable solutions. Experienced in working with cloud platforms like AWS and GCP, as well as SQL for data extraction and analysis. Passionate about transforming raw data into actionable insights through advanced analytics, statistical modeling, and visualization. Strong understanding of MLOps principles to streamline deployment and monitoring of models in production. Proven ability to collaborate with cross-functional teams, communicate findings effectively, and translate business problems into data-driven solutions. Holds a Bachelor’s or Master’s degree in Computer Science, Data Science, or a related field, with hands-on experience in AI research and real-world machine learning applications. Always eager to stay up-to-date with the latest advancements in artificial intelligence, deep learning, and big data technologies. Looking to leverage expertise in data science to solve complex problems and drive innovation in a dynamic, growth-oriented environment. Recognized for analytical problem-solving, creativity, and ability to deliver high-impact solutions under tight deadlines. Enthusiastic about contributing to data-driven decision-making and shaping the future of AI-driven solutions. Open to exciting opportunities in AI, analytics, and machine learning engineering roles.\"\n",
    "resume_text = df['Resume_str'][0]\n",
    "print(f\"Match Score: {match_resume_to_job(resume_text, job_desc)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This resume needs significant restructuring and refinement to be more effective.  The current version is cluttered and doesn't effectively highlight the candidate's key skills and accomplishments. Here's a suggested improvement plan:\n",
      "\n",
      "**1.  Rewrite the Summary:**\n",
      "\n",
      "The current summary focuses on hospitality, which isn't directly relevant to the target HR Administrator/Marketing Associate role.  It needs to be concise and highlight relevant skills and experience for the desired position.  For example:\n",
      "\n",
      "> *Highly accomplished and results-oriented HR Administrator and Marketing Associate with 15+ years of experience in driving operational efficiency and enhancing brand presence. Proven ability to manage employee relations, administer benefits programs, develop marketing campaigns, and build strong customer relationships.  Seeking a challenging role leveraging expertise in both HR and Marketing to contribute to organizational growth.*\n",
      "\n",
      "\n",
      "**2.  Consolidate Experience:**\n",
      "\n",
      "The resume lists many jobs, making it difficult to quickly grasp the candidate's career progression and key achievements.  Combine roles with similar responsibilities, focusing on quantifiable achievements and using action verbs.  Use the PAR (Problem-Action-Result) method to describe accomplishments.\n",
      "\n",
      "**Example of consolidated experience (combining several roles):**\n",
      "\n",
      "> **HR Administrator/Marketing Associate | Company Name | City, State | Dec 2013 – Present**\n",
      "> *Spearheaded the redesign and launch of the company website in under two months, resulting in a 20% increase in lead generation (quantify if possible).*\n",
      "> *Developed and implemented a new employee onboarding program, reducing training time by 15% and improving employee satisfaction scores by 10% (quantify if possible).*\n",
      "> *Managed all aspects of employee benefits administration, including health insurance, retirement plans, and leave of absence programs.*\n",
      "> *Created and managed marketing collateral for sales meetings, trade shows, and executive presentations, increasing brand awareness by X% (quantify if possible).*\n",
      "> *Successfully resolved X number of employee relations issues, preventing potential legal complications (quantify if possible).*\n",
      "> *Managed social media presence, resulting in Y% increase in followers and engagement (quantify if possible).*\n",
      "\n",
      "\n",
      "**3.  Focus on Quantifiable Results:**\n",
      "\n",
      "Throughout the resume, add numbers and data to demonstrate the impact of the candidate's work.  Instead of \"Improved customer service,\" say \"Improved customer satisfaction scores by 15%.\"\n",
      "\n",
      "**4.  Tailor to the Job Description:**\n",
      "\n",
      "Before submitting, carefully review the job description for each position applied to.  Highlight the skills and experiences that directly align with the requirements.  Use keywords from the job description.\n",
      "\n",
      "**5.  Streamline Skills:**\n",
      "\n",
      "The current skills section is a long list of keywords.  Categorize them into relevant skill sets (e.g., HR Administration, Marketing, Customer Service, Financial Management) and use stronger action verbs.\n",
      "\n",
      "\n",
      "**6.  Improve Formatting:**\n",
      "\n",
      "Use a clean, professional resume template.  Ensure consistent formatting throughout.  Use bullet points effectively to break up large blocks of text.\n",
      "\n",
      "\n",
      "**7.  Education Section:**\n",
      "\n",
      "Add the degree type (e.g., Associate's Degree) for your education.  Consider removing the High School Diploma unless it's relevant to a specific job requirement.\n",
      "\n",
      "\n",
      "**8.  Remove Redundancies:**\n",
      "\n",
      "Many phrases are repeated across different job descriptions.  Consolidate information and avoid unnecessary repetition.\n",
      "\n",
      "\n",
      "**9.  Proofread Carefully:**\n",
      "\n",
      "Before submitting, thoroughly proofread for any grammatical errors or typos.\n",
      "\n",
      "\n",
      "By implementing these changes, the resume will be much more impactful and effectively showcase the candidate's qualifications for HR Administrator/Marketing Associate positions.  Remember to quantify achievements whenever possible and tailor the resume to each specific job application.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "genai.configure(api_key=os.environ[\"GEMINI_API_KEY\"])\n",
    "\n",
    "def improve_resume(resume_text):\n",
    "    model = genai.GenerativeModel(\"gemini-1.5-flash\") \n",
    "    prompt = f\"Analyze this resume and suggest improvements:\\n{resume_text}\"\n",
    "    response = model.generate_content(prompt)\n",
    "    return response.text\n",
    "\n",
    "# Test on a sample resume\n",
    "print(improve_resume(df['Resume_str'][0]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
